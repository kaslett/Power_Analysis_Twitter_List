Treat_sample_3,
variable_name,
'PA_News_Discernment.png')
df_1$diff_aff_pol
#Name variable:
variable_name <- 'Affective Polarization'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$aff_pol_w1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$aff_pol_w1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
mean(Control_sample_1)
sd(Control_sample_1)
#Name variable:
variable_name <- 'Affective Polarization'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$aff_pol_w1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$aff_pol_w1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation:
mean(Control_sample_1)
sd(Control_sample_1)
sd(Control_sample_1)*0.1
sd(Control_sample_1)*0.15
sd(Control_sample_1)*0.2
#Name variable:
variable_name <- 'Affective Polarization'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$aff_pol_w1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$aff_pol_w1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation:
mean(Control_sample_1)
sd(Control_sample_1)
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- na.omit(df_1$aff_pol_w1)
#Treatment:
Treat_sample_2 <- na.omit(df_1$aff_pol_w1)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- na.omit(df_1$aff_pol_w1)
#Treatment:
Treat_sample_3 <- na.omit(df_1$aff_pol_w1)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'PA_Affective_Polarization.png')
#Name variable:
variable_name <- 'Political Cynicism'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$Pol_cyn_1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$Pol_cyn_1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- na.omit(df_1$Pol_cyn_1)
#Treatment:
Treat_sample_2 <- na.omit(df_1$Pol_cyn_1)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- na.omit(df_1$Pol_cyn_1)
#Treatment:
Treat_sample_3 <- na.omit(df_1$Pol_cyn_1)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'./Figures/PA_Political_Cynicism.png')
df_1$Trust_Media_w1
#Name variable:
variable_name <- 'Trust in Mainstream News'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$Trust_Media_w1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$Trust_Media_w1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- na.omit(df_1$Trust_Media_w1)
#Treatment:
Treat_sample_2 <- na.omit(df_1$Trust_Media_w1)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- na.omit(df_1$Trust_Media_w1)
#Treatment:
Treat_sample_3 <- na.omit(df_1$Trust_Media_w1)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'./Figures/PA_Trust_Mainstream_News.png')
#Name variable:
variable_name <- 'Political Cynicism'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$Pol_cyn_1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$Pol_cyn_1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation of variable:
mean(Control_sample_1)
sd(Control_sample_1)
sd(Control_sample_1)*0.2
sd(Control_sample_1)*0.15
#Name variable:
variable_name <- 'Trust in Mainstream News'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$Trust_Media_w1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$Trust_Media_w1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation of variable:
mean(Control_sample_1)
sd(Control_sample_1)
sd(Control_sample_1)*0.2
Control_sample_1 <- rnorm(100, 5, 1)
sd(Control_sample_1)
rnorm(10000, 5, 1)
sd(Control_sample_1)
Control_sample_1 <- rnorm(10000, 5, 1)
sd(Control_sample_1)
#Name variable:
variable_name <- 'Misinformation Literacy'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- rnorm(100000, 5, 1)
#Treatment:
Treat_sample_1 <- rnorm(100000, 5, 1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation of variable:
mean(Control_sample_1)
sd(Control_sample_1)
#Name variable:
variable_name <- 'Misinformation Literacy'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- rnorm(100000, 5, 1)
#Treatment:
Treat_sample_1 <- rnorm(100000, 5, 1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation of variable:
mean(Control_sample_1)
sd(Control_sample_1)
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- rnorm(100000, 5, 1)
#Treatment:
Treat_sample_2 <- rnorm(100000, 5, 1)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- rnorm(100000, 5, 1)
#Treatment:
Treat_sample_3 <- rnorm(100000, 5, 1)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'./Figures/PA_Misinformation_Literacy.png')
#Load libraries:
library(ggplot2)
library(dplyr)
#Create function to create power analysis chart:
power_analysis <- function(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
name_plot) {
#Create sequence of possible total sample numbers
num_to_sample <- seq(100, 3000, by =100)
#Create empty matrix:
All_Data <- matrix(ncol=3)
#For loop that runs for each sample number for Cohen's D = 0.10:
for(x in 1:length(num_to_sample)){
#Empty vector for P_values:
P_values <- c()
#For loop with number of simulations:
for(z in 1:1000){
#Sample from predicted control data half of the total sample to be placed in the control data:
Control_data <- sample(Control_sample_1, size=num_to_sample[x]/2, replace = T)
#Sample from predicted treatment data half of the total sample to be placed in the treatment data:
Treat_data <- sample(Treat_sample_1, size=num_to_sample[x]/2, replace = T)
#Create vector with control and treatment data:
Prop_FC <- c(Control_data,Treat_data)
#Create treatment variable (1=assigned treatment ; 0 = assigned control group)
C_treatment <- sample(0, size=num_to_sample[x]/2, replace = T)
T_treatment <- sample(1, size=num_to_sample[x]/2, replace = T)
#Create vector with control and treatment assignment variable:
assign_treatment <- c(C_treatment,T_treatment)
#Create dataframe with both veriable data and treatment assignment
Final_data <- data.frame(Prop_FC, assign_treatment)
#Run Average treatment effect without controlling covariates:
lm_results <- lm(data=Final_data,Prop_FC ~ assign_treatment)
#Append P-value of effect size to the P_Values vector:
P_values <- c(P_values,summary(lm_results)$coefficients[2,4])
}
#Create proportion of P-values that are below 0.05
Above_95 <- ifelse(P_values < 0.05,1,0)
Perc_Above_95 <- mean(Above_95)
#Add sample number and proportion of P-values that are below 0.05
All_Data <- rbind(All_Data,c(num_to_sample[x],Perc_Above_95,0.10))
}
#For loop that runs for each sample number for Cohen's D = 0.10:
for(x in 1:length(num_to_sample)){
#Empty vector for P_values:
P_values <- c()
#For loop with number of simulations:
for(z in 1:1000){
#Sample from predicted control data half of the total sample to be placed in the control data:
Control_data <- sample(Control_sample_2, size=num_to_sample[x]/2, replace = T)
#Sample from predicted treatment data half of the total sample to be placed in the treatment data:
Treat_data <- sample(Treat_sample_2, size=num_to_sample[x]/2, replace = T)
#Create vector with control and treatment data:
Prop_FC <- c(Control_data,Treat_data)
#Create treatment variable (1=assigned treatment ; 0 = assigned control group)
C_treatment <- sample(0, size=num_to_sample[x]/2, replace = T)
T_treatment <- sample(1, size=num_to_sample[x]/2, replace = T)
#Create vector with control and treatment assignment variable:
assign_treatment <- c(C_treatment,T_treatment)
#Create dataframe with both veriable data and treatment assignment
Final_data <- data.frame(Prop_FC, assign_treatment)
#Run Average treatment effect without controlling covariates:
lm_results <- lm(data=Final_data,Prop_FC ~ assign_treatment)
#Append P-value of effect size to the P_Values vector:
P_values <- c(P_values,summary(lm_results)$coefficients[2,4])
}
#Create proportion of P-values that are below 0.05
Above_95 <- ifelse(P_values < 0.05,1,0)
Perc_Above_95 <- mean(Above_95)
#Add sample number and proportion of P-values that are below 0.05
All_Data <- rbind(All_Data,c(num_to_sample[x],Perc_Above_95,0.15))
}
#For loop that runs for each sample number for Cohen's D = 0.10:
for(x in 1:length(num_to_sample)){
#Empty vector for P_values:
P_values <- c()
#For loop with number of simulations:
for(z in 1:1000){
#Sample from predicted control data half of the total sample to be placed in the control data:
Control_data <- sample(Control_sample_3, size=num_to_sample[x]/2, replace = T)
#Sample from predicted treatment data half of the total sample to be placed in the treatment data:
Treat_data <- sample(Treat_sample_3, size=num_to_sample[x]/2, replace = T)
#Create vector with control and treatment data:
Prop_FC <- c(Control_data,Treat_data)
#Create treatment variable (1=assigned treatment ; 0 = assigned control group)
C_treatment <- sample(0, size=num_to_sample[x]/2, replace = T)
T_treatment <- sample(1, size=num_to_sample[x]/2, replace = T)
#Create vector with control and treatment assignment variable:
assign_treatment <- c(C_treatment,T_treatment)
#Create dataframe with both veriable data and treatment assignment
Final_data <- data.frame(Prop_FC, assign_treatment)
#Run Average treatment effect without controlling covariates:
lm_results <- lm(data=Final_data,Prop_FC ~ assign_treatment)
#Append P-value of effect size to the P_Values vector:
P_values <- c(P_values,summary(lm_results)$coefficients[2,4])
}
#Create proportion of P-values that are below 0.05
Above_95 <- ifelse(P_values < 0.05,1,0)
Perc_Above_95 <- mean(Above_95)
#Add sample number and proportion of P-values that are below 0.05
All_Data <- rbind(All_Data,c(num_to_sample[x],Perc_Above_95,0.20))
}
colnames(All_Data) <- c('Sample_Size','Power','Effect_Size')
All_Data <- data.frame(All_Data)
All_Data$Effect_Size <- as.character(All_Data$Effect_Size)
All_Data <- na.omit(All_Data)
ggplot(All_Data,aes(y=Power,x=Sample_Size,group=Effect_Size)) +
geom_line(size=2,aes(color=Effect_Size)) +
theme_bw() +
geom_hline(yintercept=0.8, linetype="dashed",
color = "black", size=2) +
ylab('Power') +
xlab('Sample Size') +
ggtitle(variable_name) +
scale_color_discrete(name = "Cohen's D")
ggsave(name_plot,width = 14, height = 8)
}
#Read in data from previous similar project:
df_1 <- read.csv('Clean_NewsGuard_Survey_Study.csv')
df_2 <- read.csv('Clean_NewsGuard_Digital_Trace_Data.csv')
#Filter out treatment groups:
df_1 <- df_1 %>% filter(Treated == 0)
df_2 <- df_2 %>% filter(Treated == 0)
#Set seed:
set.seed(749)
################################################################
########### Hypothesis 1: Discernment  of News Headlines  ######
################################################################
#Name variable:
variable_name <- 'Discernment  of News Headlines'
#Create our measure using previous data:
df_1$Covid_Misinfo_revised = (df_1$Covid_Misinfo_Index_w2 - 3)*-1
df_1$Covid_info_revised = df_1$Covid_info_Index_w2
df_1$BLM_Misinfo_revised = (df_1$BLM_Misinfo_Index_w2 - 3)*-1
df_1$BLM_info_revised = df_1$BLM_info_Index_w2
df_1$Covid_Misinfo_revised = df_1$Covid_Misinfo_revised*3
df_1$Covid_info_revised = df_1$Covid_info_revised*2
df_1$BLM_Misinfo_revised = df_1$BLM_Misinfo_revised*3
df_1$BLM_info_revised = df_1$BLM_info_revised*2
df_1$discernment_index = (df_1$Covid_Misinfo_revised+df_1$Covid_info_revised+df_1$BLM_Misinfo_revised+df_1$BLM_info_revised)/10
#Mean and standard deviation of data:
mean(df_1$discernment_index,na.rm=T)
sd(df_1$discernment_index,na.rm=T)*0.15
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$discernment_index)
#Treatment:
Treat_sample_1 <- na.omit(df_1$discernment_index)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- na.omit(df_1$discernment_index)
#Treatment:
Treat_sample_2 <- na.omit(df_1$discernment_index)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- na.omit(df_1$discernment_index)
#Treatment:
Treat_sample_3 <- na.omit(df_1$discernment_index)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'./Figures/PA_News_Discernment.png')
################################################################
############ Hypothesis 5: Affective Polarization  #############
################################################################
#Name variable:
variable_name <- 'Affective Polarization'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$aff_pol_w1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$aff_pol_w1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation:
mean(Control_sample_1)
sd(Control_sample_1)
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- na.omit(df_1$aff_pol_w1)
#Treatment:
Treat_sample_2 <- na.omit(df_1$aff_pol_w1)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- na.omit(df_1$aff_pol_w1)
#Treatment:
Treat_sample_3 <- na.omit(df_1$aff_pol_w1)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'./Figures/PA_Affective_Polarization.png')
################################################################
#################### Hypothesis 5:  Political Cynicism  ########
################################################################
#Name variable:
variable_name <- 'Political Cynicism'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$Pol_cyn_1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$Pol_cyn_1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation of variable:
mean(Control_sample_1)
sd(Control_sample_1)
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- na.omit(df_1$Pol_cyn_1)
#Treatment:
Treat_sample_2 <- na.omit(df_1$Pol_cyn_1)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- na.omit(df_1$Pol_cyn_1)
#Treatment:
Treat_sample_3 <- na.omit(df_1$Pol_cyn_1)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'./Figures/PA_Political_Cynicism.png')
################################################################
############# Hypothesis 5: Trust in Mainstream News  ##########
################################################################
#Name variable:
variable_name <- 'Trust in Mainstream News'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- na.omit(df_1$Trust_Media_w1)
#Treatment:
Treat_sample_1 <- na.omit(df_1$Trust_Media_w1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation of variable:
mean(Control_sample_1)
sd(Control_sample_1)*0.2
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- na.omit(df_1$Trust_Media_w1)
#Treatment:
Treat_sample_2 <- na.omit(df_1$Trust_Media_w1)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- na.omit(df_1$Trust_Media_w1)
#Treatment:
Treat_sample_3 <- na.omit(df_1$Trust_Media_w1)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'./Figures/PA_Trust_Mainstream_News.png')
################################################################
############# Hypothesis 5: Misinformation Literacy ############
################################################################
#Name variable:
variable_name <- 'Misinformation Literacy'
#Predicted data:
#Smallest Effect Size (Cohen's D = 0.1):
#Control:
Control_sample_1 <- rnorm(100000, 5, 1)
#Treatment:
Treat_sample_1 <- rnorm(100000, 5, 1)
Treat_sample_1 <- Treat_sample_1 + sd(Control_sample_1)*0.1
#Mean and standard deviation of variable:
mean(Control_sample_1)
sd(Control_sample_1)
#Medium Effect Size (Cohen's D = 0.15):
#Control:
Control_sample_2 <- rnorm(100000, 5, 1)
#Treatment:
Treat_sample_2 <- rnorm(100000, 5, 1)
Treat_sample_2 <- Treat_sample_2 + sd(Control_sample_2)*0.15
#Largest Effect Size (Cohen's D = 0.20):
#Control:
Control_sample_3 <- rnorm(100000, 5, 1)
#Treatment:
Treat_sample_3 <- rnorm(100000, 5, 1)
Treat_sample_3 <- Treat_sample_3 + sd(Control_sample_3)*0.2
power_analysis(Control_sample_1,
Treat_sample_1,
Control_sample_2,
Treat_sample_2,
Control_sample_3,
Treat_sample_3,
variable_name,
'./Figures/PA_Misinformation_Literacy.png')
